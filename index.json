[{"body":"","link":"https://phpscaling.com/","section":"","tags":null,"title":""},{"body":"Hello. I'm Alister Bulman (ode to the usefulness of a unique name). I live in West London, but I lived for most of my life in Bournemouth, on the south coast of the UK.\nThis is the home for my technical discussions, suggestions, links to some very interesting posts I've seen elsewhere (hopefully good), and more than the odd rant, about other developers, technologies and anything else that's vaguely related.\nI've been programming since 1981, when I first came across a Commodore PET in my classroom, and since 1987, I've pretty much made my entire living from programming - and with a long-time interest in the Internet in general, though it was to be several years after I got interested in online communication that I started writing websites (mostly because the World Wide Web hadn't been invented when I first started online).\nI've taken a couple of Sabbaticals between jobs, most recently in December to January 2011-12, and previously a full seven months in 2009. I find it useful to rest, and learn some new skills.\nNow, as of the end of June 2023, I am back on the market, and looking for fully-remote back-end PHP/Symfony roles.\nLinks to my other work, including Stackoverflow (and my extended CV there), Github, LinkedIn, twitter, etc are available via Contacts.\n","link":"https://phpscaling.com/about/","section":"","tags":null,"title":"About"},{"body":"I can be contacted in a number of ways.\nPreferred: Email abulman@gmail.com Mastodon: https://mastodon.cloud/@Alister Twitter: @alister_b Stack Overflow abulman.co.uk Or just Google me. One of the advantages of a distinctive name, I'm very, very searchable. In fact, Google can even correct misspellings. Do note however, there are people with similar – but not exact – names. Some of these people are not nice, and so it is very important to me that you spell 'Alister' correctly. If I mis-named you, that would be disrespectful and I would want to be correct, and respectful, I would just ask you to do the same for me.\n","link":"https://phpscaling.com/contact/","section":"","tags":["Main"],"title":"Contact Me"},{"body":"","link":"https://phpscaling.com/tags/main/","section":"tags","tags":null,"title":"Main"},{"body":"","link":"https://phpscaling.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://phpscaling.com/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://phpscaling.com/tags/advanced/","section":"tags","tags":null,"title":"advanced"},{"body":"","link":"https://phpscaling.com/tags/graphdb/","section":"tags","tags":null,"title":"graphdb"},{"body":" Thus far, I've not done anything serious with much more with database like Mysql, some Postgres and on the NoSql side MongoDB (with a frisson of some use of Redis for some barely-more-than basic things), but I saw some mention of using RedisGraph PHP Client back in late October, as part of my regular scan of the packagist feed for new PHP/Composer packages.\nThe 'kjdev/redis-graph' package is the first example of an interface library to RedisGraph - an extension module to Redis that became possible with Redis 4.0's release. Other modules now include Bloom filters, rate-limiting and a JSON type.\nThis caught my interest. I've been a fan and user of Redis for several years, and it's now a go-to tool that I use for most of my caching needs. I've not used it as a 'serious' database yet though, considering it more as a transient cache. That opinion is changing fast though.\nI was curious about a few things:\nHow fast to run a useful query? How fast is it to insert new (bulk) data? How much space does data take up? Getting RedisGraph running / Why its different to other GraphDBs The first part was easy enough. Although I didn't have a version of Redis at that time (a couple of weeks ago) that could use Redis Modules, they have a Docker image that can easily be used for testing.\n1docker run -p 6380:6379 -it --rm redislabs/redisgraph As I already had an instance of Redis-server running, I'm exporting this new version on a different port - port 6380 outside of Docker, from the usual 6379 inside. That allowed me to keep running my current (old) version, but try the new version with redis-cli -p 6380. You can try the same examples and get a feel for it.\nSparsity is 74%, and its density is 26% A (small) part of the reason I'd not tried much with GraphDBs is because I heard that they can be slow - but that RedisGraph used a set of new techniques that were previously far more difficult. A sparse matrix is a data-structure that could have thousands of items in a grid, but only a small number of the intersections between them might be used. A naive storage mechanism for thousands of possible points could take up hundreds of megabytes of RAM - far from ideal when most of the points that could be connected, are not.\nRedisGraph uses a sparse matrix for each possible combination meaning that the simple query of finding out that a connection (an 'edge' in graph-parlance) has been made between items (a 'node'). Each node, and edge can also have data associated between them - not just a simple name, but a group of properties - any of which can be used to search for the node, or edge (item/record, or connection). Even more interestingly, those matrices can have various mathematical operations performed on them to calculate some very complex potential relationships. This avoids entirely the need to search and traverse the nodes to be able to make queries.\nPushing data So - I set about writing 'Caxton' as a personal test and a basic example of creating and querying a large number of items in a graph. This is deliberately simple for now - just a set of people, with some randomly generated data attached ('username', 'full name' and a couple of dates, stored as an integer), that can be linked to another user. The links also have a date ('createdAt' time, and a simple key:value).\nThe project is online at github - https://github.com/alister/caxton written in PHP, and as a simple console command. `bin/console app:create-graph 750 --prime=15`. It is written as a single simple class, called from a Symfony/Console command, in a small Symfony application and using the kjdev/redis-graph library to talk to the Redis Module.\nbuildPersons is called to create as many 'node' items as is needed - I call each one a 'Person'. An internal loop creates a single Node, and then 'yield's it to the outside loop, where it's stored in an array for now.\nEach node added is passed to the library to store, and then is committed to the database in groups of up to 128. The first 1% of the users are marked as a little special - they can connect to the rest. I pick a random number of the other 'Persons' (up to 2,500), and then they are linked:\n1MATCH (r:Person),(c:Person) WHERE r.username = \u0026#39;{$username}\u0026#39; AND c.username = \u0026#39;{$linkToPerson}\u0026#39; 2CREATE (r)-\\[:link\\]-\u0026gt;(c) 3# Adding an index on \u0026#39;username\u0026#39; speeds up this search immensely. Where I create the [link] clause, I also add some other properties. I'm not currently using it, but with them in place, I could ask later (for example) 'When was Person r, connected to person c?', or search for links dated before, or after a particular time.\nThe variables inside the query are having to be interpolated in place, and this does sadden me somewhat from a security point of view, and to a lesser extent, ease of use. As Neo4j has a great deal more time of development, for some years it has had the ability to use parameterised queries - and that also helps for an impressive speed boost. I hope that RedisGraph could add them as well in the future (though I wonder if an optional binary format would have to be added to enable it with the mostly-text-oriented Redis commands). It has also one of the techniques that makes SQL injection in PHP, or any other language, much more difficult to even allow to happen accidentally.\n1\\# bin/console app:create-graph 75000 --prime=150 2 3\\\u0026gt; Building person nodes. 4\u0026gt; 75000/75000 \\[============================\\] 100% 5\u0026gt; Build connections. 6\u0026gt; 104832/196641 \\[==============\u0026gt;-------------\\] 53% Running queries Finding the number of links to each user (even with over 195,000 such links) is surprisingly fast - 1411.8ms, just under 1.5 seconds.\n1$query = \u0026#39;MATCH (r:Person)-\\[x:link\\]-\u0026gt;(:Person) RETURN r.username,COUNT(x)\u0026#39;; 2$result = $graph-\u0026gt;query($query); Total number of links\nr.username COUNT(x) alexandre44 1679.000000 alejandra68 172.000000 alison37 1302.000000 alivia.oconner 1300.000000 alverta32 2162.000000 amalia.orn 306.000000 astehr 809.000000 avis.hand 2121.000000 annabell94 806.000000 annetta08 157.000000 acole 690.000000 etc... Finding all the links from a particular person took just 20.9ms in this example - including retrieving and parsing the results to something that can be easily displayed, or used.\n1MATCH (r:Person)-\\[:link\\]-\u0026gt;(c:Person) WHERE r.username = \u0026#39;{$username}\u0026#39; 2RETURN r.username,c.username,c.name,c.setDate\u0026#34;; All links from cartwright.malika\nr.username c.username c.name c.setDate cartwright.malika oherman Gerard Haley 0.000000 cartwright.malika arch54 Frederic Koss 0.000000 cartwright.malika boyle.glennie Aliza Anderson 1530409683.000000 cartwright.malika malvina11 Shanie Lindgren 0.000000 cartwright.malika abigail42 Cleta Lind MD 1484453700.000000 etc.... Even with a more complex search, (between two dates, stored as INTs) and an ORDER BY, returning 273 results out of 901 links from this user, the total time to search and return the data is just 5.4ms, though the library also reports an internal time taken of just over 3.85ms.\n1$today = 1544031124; // epoch, Dec 05 2018 17:32:04 2$query = \u0026#34;MATCH (r:Person)-\\[:link\\]-\u0026gt;(c:Person) 3 WHERE (r.username=\u0026#39;{$username}\u0026#39;) 4 AND (c.setDate \u0026gt; 0) AND (c.setDate \u0026lt; {$today}) 5 RETURN r.username,c.username,setDate,c.updatedAt 6 ORDER BY r.username,c.username,c.setDate\u0026#34;; r.username c.username c.setDate c.updatedAt ibernhard adriel.gleason 1524731283.000000 1526763937.000000 ibernhard aheidenreich 1525008374.000000 1496206048.000000 ibernhard aileen.schumm 1503427894.000000 1504939394.000000 ibernhard alberto90 1540786524.000000 1487852712.000000 ibernhard alexys.stokes 1494405825.000000 1498136432.000000 ibernhard allene.weber 1503973246.000000 1500810582.000000 etc Memory use My third question was \u0026quot;How much space does data take up?\u0026quot;, and just after the script has run, Redis reports \u0026quot;used_memory\u0026quot; =\u0026gt; 65,966,216 or about ~63MB.'. (My almost empty Redis instance - just after a restart - reports used_memory:876704, 856.16K in comparison).\nA little more interesting is after the server is restarted with the sample data inserted - all the data is still there, thanks to the regular dumps to disk, but now the server reports a lot less memory used: used_memory_human:32.82M.\nSpeeding up insertion There is no doubt at all that adding indexes to at least the main data-node properties speeds up searching for the data. If you are, as I am at the moment, building all the nodes and then adding links to them, with a quick test of adding ~6,200 connections/edges, the amount of time spent went from 8.8 seconds to 4.65 secs.\nCREATE INDEX ON :Person(username)' CREATE INDEX ON :Person(setDate) As with other databases indexes should be used carefully - as if they aren't being regularly used, they will have an overhead, and that means a cost.\nThere is a bulk-import command, though the format isn't well described and it does have a number of downsides (such as some complexity to setup) and it can't, at least currently, add properties to nodes or edges between them. I'll be trying that soon.\nSummary With my tests, RedisGraph looks to be more than fast enough to create useful graph database today, and to query them in some potentially complex ways. Even running on a low-spec home server, it's adding several thousands of nodes, and then connecting them with more than a thousand edges per second. With a higher-spec, more production ready system, I'd expect to see many multiples of that. The limitations are well understood, with currently incomplete support for the Cypher query language, and the simple fact that Redis runs from memory (but can easily save that to disk for persistence).\nCounts/Duration persons built 75,000 total 'edges' 181,215 build persons duration 13,848.4 ms build persons per/sec 5,415.79 build edges per/sec 1,287.92 build-edges duration 140,703.5 ms search links duration 18.7 ms This looks to be a great tool, mixing the flexibility of graph databases, NoSQL and Redis for the in-memory databases for a surprising amount of data in a quite small amount of (RAM) space - while keeping the functionality that Redis is so well known for with data-structures like the hash for 'NoSql' style schema-less data storage and retrieval.\nI'm looking forward to using it for some interesting things.\n","link":"https://phpscaling.com/post/investigating-redisgraph/","section":"post","tags":["advanced","graphdb","redis","tools"],"title":"Investigating RedisGraph"},{"body":"","link":"https://phpscaling.com/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://phpscaling.com/tags/redis/","section":"tags","tags":null,"title":"redis"},{"body":"","link":"https://phpscaling.com/tags/tools/","section":"tags","tags":null,"title":"tools"},{"body":"","link":"https://phpscaling.com/tags/php/","section":"tags","tags":null,"title":"php"},{"body":"","link":"https://phpscaling.com/tags/phpunit/","section":"tags","tags":null,"title":"phpunit"},{"body":"","link":"https://phpscaling.com/series/phpunit/","section":"series","tags":null,"title":"phpunit"},{"body":"","link":"https://phpscaling.com/series/","section":"series","tags":null,"title":"Series"},{"body":"Slow running tests are a bug - they stop you from doing as much as you can with your code, \u0026amp; its quality. Spend a little time working on making your tests better, clearer, and faster - and you'll reap rewards from your work.\nI've had a couple of useful improvements in the time spent running my PHPunit tests recently.\nFirst was avoiding setting up the database in each test, and using the Alice library (via AliceBundle) to pre-create at least the majority of the data my tests will need. The data itself is mostly produced with the Faker Library.\nOne issue to be careful to avoid with this technique is to make sure that any new records that are written are unique - often dynamically creating such things as email addresses using either random numbers, or possibly an element of the current time. Reading those records back needs to know what you expect, but that is easily solved by storing the original value into a variable to check later.\nIts all well having a clean and defined set of data when you start a test-run, but isolating the data between tests is also important - starting from the same place for everything means the underlying state is consistent. Now, it's easy enough to clear the database down before every test and rebuild it fresh, but most databases (certainly relational DBs like MySQL, Postgres and Sqlite) all have transactions, and the dama/doctrine-test-bundle for doctrine plugs into PHPUnit to begin a transaction before a test-run, and rolls-back to undo everything from the test. This keeps most, if not all the changes in memory, and serves as a quick 'undo' for what would be the database changes from each test. For debugging the tests, and seeing the state of the database during, or after the changes, you can add an explicit call to have the database `COMMIT` the changes from the transaction to examine the state of the database, knowing that a fresh-run of the tests will empty the database and recreate it with more data to your specification.\nAnother issue with that is being unsure as to find something by (usually a primary key) - emptying the table before you start creating test data is usually done with the MySQL command `TRUNCATE` command, but this doesn't reset the primary key IDs. I think this is actually something of a hidden positive - and easily solved in your tests by searching for the known data before updating data and writing it back. I call it as a hidden positive because in the live environment you'll not usually have full control over primary key IDs and the matching data, so it's good to be aware of that limitation. The same search-first technique will also be required for other dynamic data if you are using the Faker library for generating data that is going to be used to search on. For example, I've got booleans in the database that might have a 50/50, or 80/20 chance of being set - with enough potential rows the the chance of none of them being set to true, (or false) is almost none, but I can run the test to check that at least some exist - and that checks the live code.\nWriting all my test-data up front saved me a lot of effort within the individual tests, time taken to run the tests, and as importantly a great deal of memory. In fact, before writing all the data up front, my unit tests used over 1 gigabyte of RAM, afterwards with database rollbacks, that has halved.\nAn even bigger win in reducing the time taken came with an improvement to XDebug v2.6.0 - extension side code-coverage-filtering. Before that, the extension was collecting all the coverage for all PHP that was being run, if it was needed or not, and then PHPunit would filter the data - after it was already collected.\nThe code coverage filter feature of @xdebug works really well! Enabling it almost cut the build time of one of our CI jobs in half. pic.twitter.com/v3VvZiNWC4\n— Arnout Boks (@arnoutboks) August 14, 2018\nFor a few years, I'd avoided using Xdebug on my development machines because of the overhead - even when it was turned off (but it still had to check its status before not doing anything more substantial). I had instead used the 'phpdbg' CLI to run the tests with code coverage. I'm still not enabling Xdebug all the time within the PHP.ini (or more likely, the /etc/php.d/cli/ directory). Instead, I load it only for code-coverage test runs. You can see how in the code below, part of my build.xml configuration.\nAs I've already said though, the biggest single win was the extension-side coverage filtering, and enabling that was just a few lines in my test-bootstrap file:\nMy bootstrap also adds other code, like setting up a fresh database (see 'dama/doctrine-test-bundle', above) or test-specific code, like a tombstone().\nMy entire full-build test can now take a little as 5 minutes - which includes 2 minutes for a large Behat test-suite (210 scenarios, over 1000 steps), 90 seconds for a full PHPunit test-coverage run (with more than 1,250 tests running 3,600 assertions) as well as a number of code-quality tools such as PHP_CodeSniffer, PHP Copy/Paste Detector and outputting a range of reports, including the coverage in HTML and XML, PHP_CodeBrowser and API/php documentation generated by Sami. I should also add that this ~5 minute run-time happens on a lower-powered, spinning magnetic disc server.\nI started using phpdbg so I didn't have to have xdebug loaded all the time. I've now set my code coverage to just add the extension as needed, and the loader adds xdebug_set_filter for 'src/' - my coverage-run times have gone from 2m 52s for phpdbg to 1m 28s for @xdebug!\n— Alister 'Blockchain emitting diode' Bulman (@alister_b) August 29, 2018\n","link":"https://phpscaling.com/post/speeding-up-your-tests/","section":"post","tags":["php","testing","tools","phpunit"],"title":"Speeding up your tests, and also your code coverage!"},{"body":"","link":"https://phpscaling.com/tags/testing/","section":"tags","tags":null,"title":"testing"},{"body":"In early 2016, I suggested an addition to the SncRedis-bundle. The project itself is an fully-featured add-on ('bundle') for Symfony framework projects to easily do a number of very useful interface functions between Symfony and the Redis database/cache. It can, for example, quickly enable all the sessions to be put into a Redis server, and also cache the Doctrine meta-information as well as anything else that the developer would like to cache.\nMy additions were to do some relatively simple things - adding tags and aliases to the services it created from the configuration - in effect, really just adding some variables to the internal Symfony environment that could be used elsewhere, if required.\nNow, it's time to show you how why I thought it was useful to add, and what you can do with it.\nYou can do a lot with Redis, and when I'm using it, I prefer to keep things as separate as I can - internal (admin) caches, vs data that would be used on the public website, keeping the website sessions data apart from emails that are waiting to send, and background jobs that are being run.\nWith Redis, and the SncRedis bundle, it's easy to keep them apart. While databases like MySql and Postgres give databases names, Redis prefers some simplicity - by default, it will create 16 numbered databases (0-15) - although telling the server to use more is just a quick config change away.\nRecording which database is being used for which purpose though, it something that usually only happens in the configuration though - and so to show how much was in each numbered database, I kept a list to be able to refer to:\n1sncRedisDBs: 2 0: Default 3 1: Cache 4 2: Admincache 5 3: Swiftmail 6 4: DoctrineMeta 7 5: Sessions 8 7: Queue That's easy enough to refer to or use, and also to keep up to date since it's right next to the configuration. But, I'm a developer that loves to write code, and hates to keep documentation up to date, so I spent a few hours coding to allow the code to document the configuration.\nThat's what the Symfony Aliases and tags allow for - it's easy to fetch the data, and get the information about the database from Redis to be shown with what I've named database - the 'alias' I've set in the configuration, and so now I can use that name to be able to find out more about the contents of the database. Here's sample output of a fairly simple Symfony command I've written. If there's no specific parameters of something to look at - it shows the list of databases and their aliases. Alternatively, I can use the name (here, 'default' or Redis dbNum:1) to get an item from the Redis database. Currently, that's just an index into a dumped list - but it's just as easy to get a named entry.\nSo, how can we get the details of the Redis clients, which refer to the databases inside Redis itself? There are a few moving pieces:\nAdd a CompilerPass - this sends the container, to some code that can collect data from it, before it is finished which will remove unused services and information. In the CompilerPass, find all the services tagged with the 'snc_redis.client' tag, and store the data required (in this instance, the Redis client, and the 'alias' that it is known as). Create a service to hold the data that the CompilerPass finds, and be able to retrieve it on demand Lets go through them step class by class.\nAdding the CompilerPass - This is quite easy - we can create and add the relevant class right in the Symfony Framework's core AppKernel.php class. You can also add the CompilerPass from within a bundle configuration, if you have any. The CompilerPass itself finds the tagged services (themselves created in SncRedis from the configuration), and arranges for them to be added to a holding service (the AppBundle\\SncRedis\\ClientsList class/service). Because each service could have more than one tag attached, we loop the list of potential tags, and have them added to the ClientList. Finally, the AppBundle\\SncRedis\\ClientsList class/service itself. A very simple store and retrieve. We can also get all the data, or just for a specifically named service, if it exists. So - now we have a service that can store the data - how to use it? Get the service (which only then fills the information), get the clients and loop around to collect the parts of the information we need!\nOnce you have the name of the alias name of the queue, it's simple to use that as part of the name of the snc_redis client service name, and so collect data about, or from the Redis Database service itself.\n","link":"https://phpscaling.com/post/sncredis-and-tagged-services/","section":"post","tags":["advanced","php","symfony"],"title":"SncRedis and tagged services"},{"body":"","link":"https://phpscaling.com/tags/symfony/","section":"tags","tags":null,"title":"symfony"},{"body":"A recent post showed how to setup Code Tombstones - but there are other , even more insidious pieces of code in a project. The code you know you aren't using now, but you wrote ahead of time - because you think it will be useful, or you have plans for it, or any one of a dozen more reasons.\nChances are - you might never get back to it, and it's just taking up important space. Disk space, or space in version control isn't worth worrying about though - it's the space in your brain that is most important for a developer.\nYou have to let it go.\nHere's the results from one of my own projects:\nBefore deleting unused code:\nClasses: 30.22% (55/182) Methods: 51.65% (515/997) Lines: 40.99% (2080/5075) After deleting code, and adding a few new tests\nClasses: 34.52% (58/168) Methods: 58.57% (523/893) Lines: 52.18% (2299/4406) ... and all it took was deleting 2,615 lines of PHP classes, tests, Twig templates and yaml configuration - none of which were being used, or were in a state to use right now. The number of unit tests also dropped from 1,003 with 4,026 assertions, down to 984 and 3,966 assertions.\nBecause of the wonders of Git, and version control... None of that code need ever be forgotten - I made a quick and simple branch - deleting-code branch and removed a block of related code (at least what I could find) per commit.\n74a2e09 - (deleting-code) Removed unused commands 622295f - Remove deprecated code 4783f79 - Removing unused ContractorsList service 822e57b - Datatable \u0026amp; DemoTable routing and Javascript e28e535 - Remove RecruitersDemo - generated fake data on the fly 131864b - Removing the Mailable \u0026amp; Mailing code. e5b53b5 - Removing most of the 'Fakes' code ce8f717 - Start deleting unused code I've also matched those commits with some issues, and plenty of searchable text that I can use to find them in the git log.\nI, for example, I decide to come back and take another look at the 'Mailable' code (which is intended to make configuring and sending emails easier) - I can just search for 'emails' or 'mailable' and revert just a single commit - and I'm right back where I need to be. But for now, I want to just do it the 'hard way' with simple SwiftMailer-related code before I decide what can be done in an easier way.\n","link":"https://phpscaling.com/post/improve-your-code-coverage-percentage-delete-code/","section":"post","tags":["php","testing"],"title":"Improve your code coverage percentage - delete code!"},{"body":"","link":"https://phpscaling.com/tags/jobs/","section":"tags","tags":null,"title":"jobs"},{"body":"While Twitter can be really annoying, sometimes it can help to promote some wonderfully simple ideas.\nOne of these came from Andrew Woods (@awoods) - a github repo called php-in-seattle. It's a simple idea - just a list of companies around a geographical area that use PHP.\nWhat it can enable is of mutual advantage to the companies, and developers that might be looking for a new job. So, I started a similar list for the London (UK) area.\nhttps://github.com/alister/php-in-london\nPHP-In-London, (UK) There are a lot of companies in London that use PHP, and it is hard to keep track. If you know of any PHP companies with offices in the (Greater) London area (within ~20 miles), add them with a quick pull request (you can also edit directly from within the Github website if you want).\nI've also added another list for recruiters that deal with a lot of PHP roles - though as they are somewhat less technically savvy, there are only a couple of entries, one of which is a recruitment startup.\nStarting the list actually helped to earn me a T-Shirt - for DigitalOcean Hacktober (which I'm actually wearing as I write this post :D).\nTo help spread the PR and Github Karma, when someone asked to add a new item to the list, I also added them as a full collaborator on the project - allowing them to add more - something that a some of them have already done - like these fine developers.\nTristan Bailey, @tristanbailey Ross Motley, @rossmotley Scott (Fatbeehive) @scottfatbeehive kungfuchris, @kungfuchris Chris Padfield, @chrispadfield Richard George, @parsingphase Craig Willis, @craigwillis85 Awesome_Bot As a developer, I know well that anything worth doing, is worth testing, so I looked around and realised that there was indeed a way to test the URLs that were in the list. It's called 'Awesome_Bot', and it is mostly aimed at the various 'Awesome ' lists that helped to inspire the list in the first place. Setting it up with Travis isn't hard, and so I've also enabled that.\nUpdated I'm now using Github Workflows, rather than Travis.\nIt has taken a little tweaking of the configuration (even this morning, when I added Travis-CI to the whitelist to allow for redirects of the SVG build status button), but it has also helped to catch a couple of problems.\nHacktoberFest 2017? Don't wait till then, though! Although HacktoberFest 2017 hasn't been announced yet, if it does happen, I'm aiming to plug the project more in early October - but that doesn't mean you have to wait until then. If you work at a company based in London (or at least easily commutable), take a look, and throw me a pull request! If you've not used Git/Github before, then let me know, and I can walk you through what to do, or worst case, just drop me an email with the relevant details and I'll add them myself on your behalf.\nOne final question for you: I'm considering a wider UK list of companies that use PHP, probably broken down by region and then larger towns/cities. If you think it's a good idea - let me know with a comment, and/or Thumbs-Up (or down) on the issue I've created for the idea.\nShould I create a separate PHP-in-UK repo?\n","link":"https://phpscaling.com/post/php-in-london-list/","section":"post","tags":["php","jobs"],"title":"PHP-in-London list"},{"body":" Version 0.9 of scheb/tombstone autoloads a file with a tombstone() function. See the bottom of the post for a fix to override that in your own code.\nIn a large project - particularly one in a dynamic language like PHP, as a project gets bigger maintaining full control of the code can be difficult. New features are written, old ones are changed or deprecated. Sometimes code is left behind, unneeded in later versions, but still in the code-base.\nThis can be even more difficult in a full-stack framework like Symfony or Laravel, where some of the code is only run via the framework - such as Event Listeners/Subscribers, or the authentication layer.\nCode test-coverage is good, but unit tests can't tell you if the code isn't being used elsewhere. You can spend time updating code, and the tests that just aren't useful.\nIn late 2015, I came across an interesting potential solution - 'Code Tombstones'. They are a more refined version of an exception or simple echo/die. In production all they do is make a log entry - 'I was here'. In a development environment, you might choose to immediately quit and complain - having the developer investigate if the code is genuinely useful or not..\nUsing a tombstone(): Something that looks like a date - it's just a string but it's useful to note when you put the tombstone call in Your name, or initials - who put it there An (optional) label They are all just simple strings that will just be output as-is if the function is called, but knowledge is power when it comes to tracking down what is happening.\nYou could also put the call into a branch (one choice of an `if` statement, for example). You can even put in into a PHP file outside of a class - though this will risk a large number of 'ordinary failures' because PHPunit (for example) will often read many files to find testing code, and so any code outside of a class in a file will be run, showing an error.\nDefining the tombstone() function This gets a little more interesting, because you may well want to have different configurations in different environments. In development or testing (including, for web-frameworks, like the Symfony app_test.php file) - a simple no-op - or you might want to send the messages produced to the console and then stop execution entirely.\nIn a production environment - the whole point is to just log and then continue.\nIn my prod-ready tombstone(), you see I also take the further step of catching any errors to then swallow them - you would not want testing code to break your live site!\nThere is one final thing I would suggest though - make sure you test that the tombstone will in fact write to the logfile on production. After a server move, I went several months not realising that any writes to the log was trying to write to a file that could not exist, but that error was being discarded. I put a quick failing-test URL that would force running the tombstone confirmed that the logfile was created as I had planned.\nThe library I use to write the logs comes from Christian Scheb, (scheb/tombstone), it takes care of the fiddly work of skipping enough of the debug_backtrace to figure out exactly where the function was called, and to log it, by as many log-handlers as you care to set.\nVideo from http://devblog.nestoria.com/post/115930183873/tombstones-for-dead-code David Schnepper's Ignite talk, \u0026quot;Isn't That Code Dead?\u0026quot; - Velocity Santa Clara 2014 Update: Dec 10th 2018 for version 0.9 As the scheb/tombstone library now autoloads a default tombstone() function, if you want to override that with a custom version (per environment for example), you'll need to (potentially) move your versions in the web/app*.php files to above the require '../vendors/autoload.php'; line.\n","link":"https://phpscaling.com/post/code-tombstones/","section":"post","tags":["php","tools"],"title":"Code Tombstones"},{"body":"One of the advantages of a side-project is that you can be a little extra passionate about getting things just right. If you want to increase code coverage because you think that it's good, you can - after all, it's just some time now doing things that you like.\nSo, earlier in the year, when I saw Sebastian Bergmann's article on 'Questioning PHPUnit Best Practices', I added it to a little (well, it currently stands at a count of 20 items...) list of clean-ups and improvements.\nWell, a couple of days ago, I had the time to take a look at it, just a simple - almost relaxing - piece of work to convert from annotations-driven Exception testing to code-driven expectation tests.\nThe idea itself is quite simple - take a piece of code like such:\n1use App\\CanThrowAnException; 2use App\\Exception\\Complicated; 3 4/** 5 * Pass the test if an exception is thrown 6 * 7 * Assert what should be happening 8 * @expectedException \\App\\Exception\\Complicated 9 */ 10function testException() 11{ 12 // setup the situation 13 $systemUnderTest = new CanThrowAnException();` 14 15 // act 16 $systemUnderTest-\u0026gt;shouldThrowComplicated(); 17} ... to the newer style, $this-\u0026gt;expectException(...), which came in with PHPUnit 5.2.\n1use App\\CanThrowAnException; 2use App\\Exception\\Complicated; 3 4/** 5 * Pass the test if an exception is thrown 6 */ 7function testException() 8{ 9 // setup the situation 10 $systemUnderTest = new CanThrowAnException(); 11 12 // assert what should be happening 13 $this-\u0026gt;expectException(Complicated::class);` 14 15 // act 16 $systemUnderTest-\u0026gt;shouldThrowComplicated(); 17} This gives a couple of advantages -\nThe test is contained entirely within the code. This means that the expectation of what should happen can be set after the initial setup has happened, giving a setup/assert/act direction of code Since it is now in PHP code, and no longer a annotation comment, it can use the namespace functionality and the ::class name resolution for shorter names, that will also still easily usable by automated PHP-parsing tools (For example, PHPStorm reads and understands the use lines to find all instances of the classes, even if they are aliased). The rewriting from the annotation to function call is almost trivial, and there are also some new (related) function calls for the other @expectedException... annotations\n1@expectedExceptionCode -\u0026gt; $this-\u0026gt;expectExceptionCode(int $code); 2@expectedExceptionMessage -\u0026gt; $this-\u0026gt;expectExceptionMessage($msgToExpect); 3@expectedExceptionMessageRegExp -\u0026gt; $this-\u0026gt;expectExceptionMessageRegExp($regexp); The last thing I did before moving on was make sure that new @expectedException annotations didn't creep back into the codebase. For that, I got a little 'hack-y'. There are probably better ways to do it (with a PHPCS sniff), but I couldn't find an easy way with a quick search - so I built my own.\nFirst of all, a little history of how I develop software, and run tests. Back in 2013 I published what I called 'personal-ci'. It is, at it's core an ant build.xmlcontrol file that runs a whole suite of tests. I'm still using the same system now, extending it very occasionally with new tools. I'll run the full set of tests maybe a couple of times a day while a system is in frequent development and that will take about five to six minutes to run. A slightly smaller subset of the unit tests will only take two seconds though!\nThe difference in, in 5.5 minutes, it will also run a full php_codesniffer report, phpcpd, over 170 Behat integration scenarios, PHPUnit with code-coverage reports, PHPMetrics reports and Sami for documentation.\nSo, in my main build.xml file - I setup a couple of searches in the src/ and tests/ directories - if the string '@expectedException' appears - fail. If one did stray back in, actually finding them is easy with ack or my own new favourite - ag.\n1\u0026lt;!-- If there are any \u0026#39;@expectedException...\u0026#39; annotations, fail. --\u0026gt; 2\u0026lt;target name=\u0026#34;noExpectedException\u0026#34;\u0026gt; 3 \u0026lt;property name=\u0026#34;fail.string\u0026#34; value=\u0026#34;@expectedException\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; 4 \u0026lt;fileset id=\u0026#34;hasexpectedException_in_src\u0026#34; dir=\u0026#34;src\u0026#34;\u0026gt; 5 \u0026lt;contains text=\u0026#34;${fail.string}\u0026#34;\u0026gt;\u0026lt;/contains\u0026gt; 6 \u0026lt;/fileset\u0026gt; 7 \u0026lt;fileset id=\u0026#34;hasexpectedException_in_tests\u0026#34; dir=\u0026#34;tests\u0026#34;\u0026gt; 8 \u0026lt;contains text=\u0026#34;${fail.string}\u0026#34;\u0026gt;\u0026lt;/contains\u0026gt; 9 \u0026lt;/fileset\u0026gt; 10 \u0026lt;fail status=\u0026#34;1\u0026#34; message=\u0026#34;One or more \u0026#39;@expectedException\u0026#39; detected\u0026#34;\u0026gt; 11 \u0026lt;condition\u0026gt; 12 \u0026lt;or\u0026gt; 13 \u0026lt;resourcecount when=\u0026#34;greater\u0026#34; count=\u0026#34;0\u0026#34; refid=\u0026#34;hasexpectedException_in_src\u0026#34;\u0026gt;\u0026lt;/resourcecount\u0026gt; 14 \u0026lt;resourcecount when=\u0026#34;greater\u0026#34; count=\u0026#34;0\u0026#34; refid=\u0026#34;hasexpectedException_in_tests\u0026#34;\u0026gt;\u0026lt;/resourcecount\u0026gt; 15 \u0026lt;/or\u0026gt; 16 \u0026lt;/condition\u0026gt; 17 \u0026lt;/fail\u0026gt; 18\u0026lt;/target\u0026gt; I've added that check to my php-linting action, which is run by default when ant is run.\n1\u0026lt;target name=\u0026#34;php-lint-ci\u0026#34; depends=\u0026#34;get-changeset.php.spacesep,noExpectedException\u0026#34; 2 if=\u0026#34;changeset.php.notempty\u0026#34; 3 description=\u0026#34;Perform syntax check of sourcecode files in parallel\u0026#34;\u0026gt; 4 \u0026lt;!-- run `php -l` on the files for a quick safety check --\u0026gt; 5\u0026lt;/target\u0026gt; So, that's the latest addition to my 'personal-ci' workflow, updating to the latest best-practices in testing exceptions with PHPUnit, and making sure that the old-style annotations can't be used in future.\n","link":"https://phpscaling.com/post/replacing-expectedexception-with-this-expectexception/","section":"post","tags":["php","testing","tools","phpunit"],"title":"Replacing @expectedException with $this-\u003eexpectException()"},{"body":"","link":"https://phpscaling.com/tags/best-practice/","section":"tags","tags":null,"title":"best-practice"},{"body":"Having just watched Sebastian Bergmann's \u0026quot;The State of PHPUnit\u0026quot; presentation from Fosdem 2015, I was inspired to install and test a project of mine with the latest stable PHPUnit - v4.7. It was easily installed on the command line.\ncomposer global require \u0026quot;phpunit/phpunit\u0026quot;\nI installed it as a new, global, tool because in my project I am using the \u0026quot;ibuildings/qa-tools\u0026quot; repository to install and help run a number of QA tools - and the stable 1.1.* versions lock PHPunit to v3.7 - the last released version of which was in April 2014.\nA good part of the reason to do so - beyond using the latest version - was also to enable the strict tests\n1\u0026lt;phpunit 2 beStrictAboutTestsThatDoNotTestAnything=\u0026#34;true\u0026#34; 3 checkForUnintentionallyCoveredCode=\u0026#34;true\u0026#34; 4 beStrictAboutOutputDuringTests=\u0026#34;true\u0026#34; 5 beStrictAboutTestSize=\u0026#34;true\u0026#34; 6 ... more parameters ... 7 colors=\u0026#34;false\u0026#34; 8 verbose=\u0026#34;true\u0026#34; 9\u0026gt; This blogpost is to help someone else that tries it - and comes across the same issue I did:\nPHP Fatal error: Class PHPUnit_Util_DeprecatedFeature_Logger contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (PHPUnit_Framework_TestListener::addRiskyTest) in ...vendor/phpunit/phpunit/PHPUnit/Util/DeprecatedFeature/Logger.php on line 201\nMy fix was simple - it took some systematic editing of the phpunit.xml file to figure it out. At first, I tried commenting out the various add-on I've got for PHPunit, tools to report slow tests, and to automatically close and check the results of any Mockery expectations. None of them helped, and so I started on the parameters in the opening XML tag of the file.\nThe actual fix was simple - the problematical line for me was:\ncolors = \u0026quot;false\u0026quot;\nRemoving that from the top of the phpunit.xml file, solved my issue, and now I've also gone on to update the \u0026quot;ibuildings/qa-tools\u0026quot; package to dev-master to get the latest-and-greatest (including automatically pulling in PHPUnit v4.* and Behat v3, among others). It was reassuring to know that I had the previous configuration safely stored in version control - so I could always just revert back to something that had worked. Running a separate copy of PHPunit installed outside of the project didn't hurt either.\nI've said for a long time that \u0026quot;you don't get paid the big bucks for knowing what to do - it's for knowing how to fix it when you make the inevitable screw-ups\u0026quot;.\nNow, when I run my PHPunit-tests, I get a lot more warnings about 'risky' tests (all of it \u0026quot;This test executed code that is not listed as code to be covered or used\u0026quot;) - but those aren't big issues for me right now.\nThe take-away is, don't be afraid to upgrade, and if there is a problem, systematically (temporarily) commenting, or removing configuration, or code, can find the issues surprisingly quickly.\n","link":"https://phpscaling.com/post/upgrading-phpunit-fixing-phpunit_util_deprecatedfeature_logger/","section":"post","tags":["best-practice","phpunit","testing","tools"],"title":"Upgrading PHPunit - fixing PHPUnit_Util_DeprecatedFeature_Logger"},{"body":"","link":"https://phpscaling.com/tags/aside/","section":"tags","tags":null,"title":"Aside"},{"body":"","link":"https://phpscaling.com/tags/beanstalkd/","section":"tags","tags":null,"title":"beanstalkd"},{"body":"","link":"https://phpscaling.com/tags/presentation/","section":"tags","tags":null,"title":"presentation"},{"body":"Just a quick note to point out a couple of presentations on Queuing. I've recently shown the second (which admittedly has some significant things in common with the original, and not just the web-based slides).\nEither way, you are welcome to look at them online, and the original html source, and some source code, are all online at http://github.com/alister\nBeanstalkd: An Introduction\nThe Good And Bad Of Queuing\nhttps://github.com/alister/Beanstalkd-Presentation\nhttps://github.com/alister/QueueGoodBad-Presentation\nhttps://github.com/alister/beanstalkd-demo\n","link":"https://phpscaling.com/presentations/","section":"","tags":["beanstalkd","Aside","presentation"],"title":"Presentations, May 2010 and July 2012, on Queueing"},{"body":"","link":"https://phpscaling.com/tags/contracting/","section":"tags","tags":null,"title":"contracting"},{"body":"","link":"https://phpscaling.com/tags/funemployment/","section":"tags","tags":null,"title":"funemployment"},{"body":"","link":"https://phpscaling.com/tags/linux/","section":"tags","tags":null,"title":"linux"},{"body":"","link":"https://phpscaling.com/tags/puppet/","section":"tags","tags":null,"title":"puppet"},{"body":"It's been one of those quiet spots around here for a while, so here's the catch-up on what has been happening while I was not posting.\nI've recently finished a short contract working with an agency, Transform (part of the Engine group) working with a couple of government departments. The Office Of The Public Guardian receives, checks and stores Lasting Powers of Attorney - a legal document that you write while still mentally compentant to say what you would like to happen should the worst occur, and by whom you want to do it. The simpler cases aren't actually very complicated, but there is a lot of work to get the form completed - and the information can have to be written in triplicate over two or three different forms.\nThe project was to work with (and at the offices of) the new Government Digital Services (GDS) who are building the http://Gov.uk project, and I helped write the first-draft (but otherwise basically complete) prototype to put the form online. If nothing else, it allows someone to step through and only have to put information in once. Myself and one other developer, with a project manager and many others from the OPG and GDS took nearly all of the 37-pages of duplicated paper forms and created a PHP/Zend Form based system, that in the end produced PDFs, ready to check and then have signed by everyone involved.\nIt was an interesting project - and it will be a valuable service to make it easier to handle, and eventually also process from the back-office perspective. It's not quite what I would normally do, I'm far more infrastructure and back-end oriented - not so used to building a large and complex flowing form, and so I elected to move on at the end of the prototype, rather than continue with the alpha/beta phases. With a little luck, it will go live later this year after some extensive user-testing to make it as useful, and easy as possible to fill in this important legal document. The end result, in a few years, should be the ability to have many times more people making an LPA for themselves, often as part as something as routine as making a will, or buying a new house.\nRest assured, there's been plenty of relaxing since I finished that project a couple of weeks ago (especially since I spend a good portion of the time working while distinctly under the weather).\nPuppet and Github Alister \u0026amp; \u0026#39;Devops\u0026#39; I'm no longer concentrating on 'Devops' and certainly not Puppet. While I do have some skills (though more recently with Ansible and some use of Kubernetes), I'm spending most of my time mostly writing PHP \u0026amp; Symfony. - Alister, June 2023\nIn the last couple of weeks, I've been a software developing machine. I've also been looking for my next contract role, hopefully something to start just after Easter - though, as I write this, exactly what that will be is still up in the air.\nFirst, I've been putting together a development VM - currently based on a beta of Ubuntu 12.04. There are a few things that go into making it.\nPuppet config https://github.com/alister/puppet-ab I've been occasionally tweaking this since before Christmas when I took some time to do a deeper dive into Puppet, after using it last year. Many of the modules I use, are actually pulled in from other github-based projects, especially from a number by https://github.com/saz (Steffen Zieger).\nPuppet-dropbox: (forked from https://github.com/cwarden/puppet-dropbox) Dropbox is a very useful tool on any desktop to copy files around your own machines, and shared folders enable easy access to others working on the same project - I found it nearly invaluable in my last contract. It was also good to be able to improve the code that was upstream - a small fix to allow for Ubuntu to be also set as a destination for the installation.\nIn the end, I have elected to use it to just install the basic command line tool (rather than the full client), and then that can be used to install the main client, if required. It saves having to store the username and password in the repository, and it is also useful from other security standpoints not having copies of your files on all machines where the puppet manifests might be run.\nDotfiles is more of a meta-project, many appear to have a repo by that name, but a large number of them are hand-rolled. I forked one of the more common bases by Ryan Bates https://github.com/ryanb/dotfiles who also runs the excellent http://railscasts.com/ (which are not all about Ruby On Rails). I have yet to find a good way to integrate it with another shell-oriented project - Oh-My-Zsh, https://github.com/robbyrussell/oh-my-zsh which is an excellent improvement over a standard Bash shell, that I've been using for more than a dozend years.\nThe common thread between both of these is to take a basic machine, with git, an SSH key and Puppet installed and bring it quickly up to a full spec development desktop/server. It's a continuing project, but a valuable one, and not just as a learning tool.\nThere are two other projects that I've been working on.\nguard-puppet-lint: Guard (see the railscast episode http://railscasts.com/episodes/264-guard) is a ruby-based project that will watch for file changes in a subdirectory hierarchy. There are a lot of plug-ins for it https://rubygems.org/search?utf8=%E2%9C%93\u0026amp;query=guard- including PHP-oriented ones for PHPunit and PHP_CodeSniffer. The project itself can be downloaded from https://rubygems.org/gems/guard-puppet-lint.\nAs the name suggests, this small ruby gem adds a slightly easier way to run Puppet-Lint through Guard. As my first released ruby-code, there's not much to it, and in fact, it's really just a hack of guard-shell, that will run puppet-lint on the changed manifests. it does make it slightly cleaner though, and so I'm happy enough. I'm also very pleased to have had a couple of (very minor) issues raised - literally one word missing from the readme file and a single character to reduce the number of false-positive files that might be processed. There are some ideas I can add to it to make it even more useful, but that can wait for a little while, and besides, I have to figure out how to better use Guard in the first place, to be able to to do so.\nMy final, and latest, project is https://github.com/alister/QR-Generator-PHP - a refactoring of the QR code library at https://github.com/edent/QR-Generator-PHP.\nThe code itself works fine, but only as a URL destination. One of the ideas that came up while working with the Office Of The Public Guardian on their new LPA form was to put QR codes onto the final output PDF pages to help verify automatically that all the pages that have been produced, have been recieved at the back office - and also being able to refer the paper form to a digital version stored in the database.\nIt's a classic refactoring though - taking a piece of code and, without changing the end results, make it possible to use in a slightly different, but useful, context. Eventually, the qr.php webpage would be a thin wrapper around the class - and the class itself could be used from backend code to, for example, generate an image that can be placed into a PDF.\n","link":"https://phpscaling.com/post/recently/","section":"post","tags":["advanced","contracting","funemployment","linux","puppet","tools"],"title":"Recently...."},{"body":"A list of the tools that I currently use or are otherwise long-term favourites.\nProgramming Language - PHP IDE - PHPstorm Text editor - SublimeText Web Server - NginX (\u0026amp; before that, Apache with mod_PHP) Database Server - Mysql / MariaDB Message Queue - beanstalkd Source Control - git Calendar App - Google Calendar Web Browser - Chrome \u0026amp; Firefox Chair - Aeron since 2006 ","link":"https://phpscaling.com/favourite-tools/","section":"","tags":null,"title":"Favourite tools"},{"body":"Recruiters: Here's the rules.\nThe first recruiter to tell me the company name, and then send the job-spec gets to forward my details - if I think it's interesting. No company name, or spec, no chance If you send my details without my OK, you lose. And I tell the company you are a loser (chances are, they are too). Sending me all the information I could want to know is good - but when you do it on spec (and probably en-mass as well), does not mean that you get to claim the bounty. Let us know what is happening. That especially includes feed back from the employers. All of the above have happened to me.\nAbout that last point about feed-back? There's one recruiter that is on my list (it's not a good list) because he didn't bother telling me what the employer was saying about me, but a different recruiter did find out and let me know. The 2nd one has still got a chance to place me, but the first, not so much. Ironically, the comments were about some rants I posted on my LinkedIn profile. It's also a potential employer I no longer care about working for.\nThe strangest story happened to me about 15 years ago. I was working with a small recruiter and spending a couple of days to tweak a CV so that it was just perfect for a potential job (this was when I still wrote CVs, this year, it's all on websites to read, not MSWord documents). Then I got a phone call from one of the largest recruitment companies in town - they had sent my CV (without my knowledge) and the employer was interested in talking to me. WTF? That was so not good. It was even worse for the small recruiter though - it turns out he knew the rogue recruiter. He was married to her.\nFinally, when you do contact a candidate with a potential role, make sure you send them your details - and about the job(s). Just a quick email with a note with the what and where. Without it, they will not know how to get back to you for anything. I know you love to talk on the phone (and it avoids that pesky audit-trail), or you might make wonderful notes in your recruitment systems for yourself, but when us developers are looking, we can get a dozen phone calls a day from all different recruiters, and quite likely on our mobile phones to boot. We've not got the chance to write it all down most of the time, so you should, and drop us a note about it. Otherwise, we can't get back to you - if it was interesting. So, it's in your own benefit to keep us in the loop.\n","link":"https://phpscaling.com/post/hire-quickly-addendum-recruiters/","section":"post","tags":["hiring","Aside","quick","quick","rants"],"title":"Hire quickly: Addendum, recruiters"},{"body":"","link":"https://phpscaling.com/tags/hiring/","section":"tags","tags":null,"title":"hiring"},{"body":"","link":"https://phpscaling.com/tags/quick/","section":"tags","tags":null,"title":"quick"},{"body":"","link":"https://phpscaling.com/tags/rants/","section":"tags","tags":null,"title":"rants"},{"body":"If you aren't taking hiring seriously - other people can, and do hire the people you need.\nI've been guilty of it before - leaving it a couple of days - or even a week before getting back to someone that sent in their CV - although of course, most of the time, it didn't matter. The person wasn't going to get hired because they were just not good enough (the generally poor quality of developers is a different rant).\nA couple of times I have been bitten hard when hiring though, such as being introduced to a sysadmin on a Thursday night, following up late Friday afternoon and finding out on Monday when I chased him up, that he had just accepted an offer.\nSo, what to do? Well, to be honest, all you can do is be swift about things. Check all CVs that come in within a couple of hours at most, and for those that show promise, get back to them and arrange the next step as quickly as you can (probably a quick chat on the phone?) and pencil in some time - in your own calendar, if not theirs - a potential time to sit down with them properly.\nPlease though, after you've had the interview get back to them quickly. Occasionally, I'll have left them with a little thing to do (some code to write, or something to get back to me on), it's a good idea to just drop a quick email confirming that after they step out the door. A couple of times when I was looking for a new job, I've actually emailed them back that afternoon, or before lunchtime the following day to follow up with some code. Both times I was starting that role inside two weeks.\nWhen I've been interviewing, I've even offered someone a job before they left the interview. It was obvious that the guy was a good developer - just searching for him online found a number of posts he'd done into relevant mailing lists. A few years later, I'd moved on myself, he was now freelancing, so on my suggestion he was interviewed again, and promptly hired again.\nThere is a cut-throat market for developers in the last few years, and that's not likely change. Really good people will always have a choice if they want it. You, as an employer, need to be worth working for.\nInteresting project(s) Enough money for that not to be an issue - though salaries for the best devs are rising fast Working conditions that don't get in the way A future post will touch more on my 'perfect wish-list' of working environments.\n","link":"https://phpscaling.com/post/hire-quickly-because-your-competitors-will/","section":"post","tags":["hiring","rants"],"title":"Hire quickly, because your competitors will."},{"body":"","link":"https://phpscaling.com/tags/fun/","section":"tags","tags":null,"title":"fun"},{"body":"A quick fun post for those of you with an Amazon Kindle - some instructions on how to a) jailbreak your reader (trivially easy), and then b) put your own wallpapers on there, so you get a more interesting 'screensaver'.\nIt's really easy, no more than 20 mins and a couple of reboots/software updates. Most of the time is literally waiting for the reader to restart after you've placed a file in the base directory.\nAll of the instructions are here: http://wiki.mobileread.com/wiki/Kindle_Screen_Saver_Hack_for_all_2.x_and_3.x_Kindles\nA few notes to make it easier to understand:\nThe current version is around 3.3. You can double-check that you have this (or later) by pressing 'Menu', and going down to 'Settings'. The version is on the bottom line: \u0026quot;Page 1 of 3 Version Kindle 3.3 (61680021)\u0026quot; You will likely need the version marked \u0026quot;*-3.2.1_install.bin\u0026quot; Once the jailbreak has been installed, you can install the screen save hack The zip file has a 'src' folder, you'll also need to copy the src/linkss/ folder to your Kindle, as /linkss/ Finally, you can add suitable wallpapers into the /linkss/screensavers/ directory. They should be greyscale .png's or .jpgs Kindle screensaverNon functional demo unit\nA quick search for \u0026quot;kindle wallpaper\u0026quot; or \u0026quot;kindle screensaver\u0026quot;, especially if you restrict it to 600x800 pixels sizes will bring up a lot of possibilities. On the right is one I made myself (I'd heard of this particular joke wallpaper before), which you are welcome to use.\n","link":"https://phpscaling.com/post/jailbreaking-your-kindle-and-putting-new-screensaver-images/","section":"post","tags":["fun","kindle","Aside","quick","quick"],"title":"Jailbreaking your Kindle, and putting new 'screensaver' images"},{"body":"","link":"https://phpscaling.com/tags/kindle/","section":"tags","tags":null,"title":"kindle"},{"body":" I was out last night at the The Big Xmas [bash] #, near Silicon Roundabout. It was a fun night out meeting various people, tech, business and recruiters. Oh, the shame though - I was wearing the same T-shirt as someone else - and, yes, I have indeed replaced people with small shell scripts.\nNow, to the main part of what this post is about - the rant. It's not aimed at the particular event last night alone though. It's alcohol at various tech-meetups in general. Look guys, you generally end up buying too much anyway, and all too often its also to the exclusion of those that may prefer to not get inebriated.\nAs an example, The Hacker News meetups will get dozens of pizzas (which are, admittedly all eaten - there are 150+ people attending usually), but also a couple of stacks worth of cans and bottles, each with 24 cans in each tray, several hundred cans at least. It's just as well they aren't all drunk on the night - many of the event-goers would be unconscious by the end. At least they will also add a few trays of soft-drinks, Lemonade and Cola.\nIf you want a couple of drinks to help lubricate the social aspect of an evening out, I've got no problem at all. I don't though. I prefer to save my brain cells for doing interesting things, like oh, writing code?\nFor other events, how about adding some more soft drinks to replace some of the alcohol? Last night, the choice was booze, or fizzy water; That was all.\nThankfully, people don't generally get blotto at the various meetups - at least that I've seen, but I expect there's been one or two that have swerved their way home sometimes.\nDo you have a comment about alcohol being served at the various meetups? Would you like more, less, or do you think that organisers and sponsors are doing it right? I would love to start a conversation here about the good or bad of it.\n","link":"https://phpscaling.com/post/booze-at-meetups/","section":"post","tags":["meetups","Aside","quick","rants"],"title":"Booze at tech meetups"},{"body":"","link":"https://phpscaling.com/tags/meetups/","section":"tags","tags":null,"title":"meetups"},{"body":"","link":"https://phpscaling.com/tags/deployment/","section":"tags","tags":null,"title":"deployment"},{"body":"Capistrano, makes deployment of code easy. If you need to do a number of additional steps as well, then the fact that they can be scripted and run automatically is a huge win.\nIf you've only got a single machine (or maybe two), then you could certainly write your own quite simple, and entirely workable system - I described something just like this in a previous post: \u0026quot;SVN checkouts vs exports for live versions\u0026quot;. That was written and used before I was deploying to multiple machines however - and had to be run from the command line of the machine itself. It was OK even when I had a couple of machines to deploy - I just opened an SSH to both, and ran the command on them both at the same time. When I attended the London Devops roundtable on Deployment I even advocated for that as a valid deployment mechanism. But, at the same time, as I was saying that (and it's in the video), I was also writing Chef cookbooks and a Capistrano script to be able to build, and then deploy code to at least four different machines at once.\nA number of people have already written about how to setup Capistrano to deploy PHP scripts. I'll not repeat their work, instead I'll just tell you some of the problems you might come across afterwards.\ncap shell is a wonderful thing, until it bites you The Capistrano shell will let you run a simple command or an internal task on one, or as many machines as you want. This can be useful when you are trying things out - and if you are in anyway unsure where a command can be run - you can practice it, just do:\n1cap\u0026gt; with web uptime 2cap\u0026gt; on host.example.com uptime Those two commands just show how long a machine has been up, and the current load average. Easy, and safe, but as they run, they show the list of machines they succeed on.\nThere are some other useful commands you can try:\n1## show the currently live REVISION file on each machine 2cap\u0026gt; cat /mnt/html/deployed/current/REVISION 3## This file is created as each new/updated checkout is done. 4## change your path to the ./current/ path as appropriate Since you should be deploying the same codebase to all your live machines at a time (or staging, or qa/test), the versions (or git sha1's) should be the same as well.\nFinally, in the 'useful' list is cap deploy:cleanup - this will remove old deployments. Keeping a few around are useful, but they can take up a lot of space. As cap --explain deploy:cleanup says:\nClean up old releases. By default, the last 5 releases are kept on each server (though you can change this with the keep_releases variable). All other deployed revisions are removed from the servers. By default, this will use sudo to clean up the old releases, but if sudo is not available for your environment, set the :use_sudo variable to false instead.\nIf you want to change the default to something other than 5, that can be set with the line \u0026quot;set :keep_releases, 10\u0026quot; in deploy.rb.\nA few gotcha's When cap shell checks the source repo version I've found that the latest version available in the main source code repository is only apparently checked when the Capistrano shell is first run. This can be useful if you want to check out to a limited set of machines, run a test and then check out to all the machines (you end up with the same version checked out in the same-named 'releases/' directory), but if you are sitting on the cap\u0026gt; prompt in Capistrano shell and doing multiple !deploy commands, you won't get new versions of code that have been committed to the repository. Exit the shell, and re-run to solve this.\nYou checked out a new version, but you can't see it Be wary if you are logged into the machine, and sitting somewhere inside the ./current/ directory. Because of the symlink is being changed underneath you to a new directory that is being pointed to (the newest subdirectory in releases/), if you do not do a cd . to refresh your location within the real directory tree, you will still be in an old copy of the code. The 'cd' makes sure you are in the latest place on disk, via the (now changed) symlink.\nRolling back Capistrano has the ability to remove the currently live version, and change the 'current' symlink to the previous location. Should the worst happen, and a website deployment fail, this can help, if 'rolling forward', with a fast-fix, check-in and redeploy may not be easily possible.\n1# to roll back to a previous deployment: 2cap\u0026gt; with !deploy:rollback If you have rolled back the webservers (php/app servers) you will have to restart php-fpm (or maybe Apache) on the servers, as they do not necessarily pick up the (old) versions of code that is being run now. The same would also be true if you have set APC to cache the byte-code and not look at the time-stamp of files in case they change. I've found that PHP-FPM also has this issue.\n","link":"https://phpscaling.com/post/deployment-with-capistrano-the-gotchas/","section":"post","tags":["best-practice","deployment","tools"],"title":"Deployment with Capistrano - the Gotchas"},{"body":"I've been pretty busy in the last couple of years, first at Binweevils and in 2011, PeerIndex - hence the utter lack of posts, but as the note on my personal CV site says, I'm taking some time off between looking for my next role. This does give the opportunity to write more of PHP Scaling and the tools around development that I've been using in the last couple of years, and that have been piquing my curiosity.\nSo, it is my plan to investigate other languages such as Python and Ruby, and tools like Puppet and Node.Js. Rest assured, I'll keep up with the state-of-the art in PHP and such technologies as MongoDB though!\nThere's also a number of planned posts right here, more for Beanstalkd (and talking about other queues), Deployment with Capistrano, graphing and logging (including how to mark a Capistrano deployment in a graph!) and a few other things, including rants.\n","link":"https://phpscaling.com/post/back-from-the-coalface/","section":"post","tags":["fun","funemployment","me","Status","quick"],"title":"Back from the coalface"},{"body":"","link":"https://phpscaling.com/tags/me/","section":"tags","tags":null,"title":"me"},{"body":"","link":"https://phpscaling.com/tags/status/","section":"tags","tags":null,"title":"Status"},{"body":"I've previously shown you why you may want to put some tasks through a queuing system, what sort of jobs you could define, plus how to keep a worker process running for as long as you would like (but still be mindful of problems that happen).\nIn this post, I'll show you how to put the messages into the queue, and we'll also make a start on reading them back out.\nFor PHP, there are two BeanstalkD client libraries available.\nBeanStalk.class.php - http://sourceforge.net/projects/beanstalk/ Pheanstalk - http://github.com/pda/pheanstalk Although I've previously used the first class in live code, I'm preferring the second, 'Pheanstalk', for this article. It is more regularly worked on, and uses object orientation to the fullest, plus it's got a test suite (based on Simpletest, which is included in the download).\nUsing it, according to the example is simple:\nThe pheanstalk_init.php file adds an autoloader, though you may find it advantageous to move the main class file hierarchy from where it had been downloaded into its own directory so that an existing (for example Zend Framework) auto-loader can find it.\nAs you see above, the Object Orientation lends itself well to (an optional) 'fluid' programming style, where an object is returned and then can be acted on in turn $pheanstalk-\u0026gt;useTube('testtube')-\u0026gt;put(\u0026quot;job payload goes here\\n\u0026quot;);\nSo, putting simple data into the queue, is, well, simple (as it should be). There are advantages in wrapping this simplicity into our own class though. Some examples\nWe want to put the same job into the queue multiple times - for example, a call to check some data in 1, 10 and 20 seconds time. Adding a new default priority - or with multiple classes, a small range of defaults adding in other (meta) information about the job that is being run, such as when it was queued, and how important it is. Some tasks might be urgent, but not important - ie, if we have the opportunity, run them now - but it doesn't have to be run at all. Each may be simple enough to create a simple loop, but it might be advantagous to push that down into a class - and especially with the final idea.\nHow to store the meta-information then? It should be a text-friendly, but concise format, and quick to parse. Here, JSON (or the related Yaml) fits the bill quite nicely.\nProcessing it at the other end, after it has been fetched by the worker is a simple matter of running 'json_decode()' and extracting the ['task'] from the results before running it.\nComments: Aaron - Jan 4, 2012\nMoving from Amazon SQS to Beanstalkd. Does $pheanstalk-\u0026gt;watch() return the oldest item added to the tube? Or does it return multiple items? Also, can $priority be null? Will this effect FIFO order? If all items have the same priority are they delivered to the worker FIFO? Thanks. I would love to see more examples of pheanstalkd.\nalister - Jan 4, 2012\nYou don't have to actively use priority, but in that case, just default it to some number (I'd personally say a fairly high number, and so make it a low-priority, in case you did want to set something as more important later). If there aren't priorities being used, then it would be plain FIFO. Different priorities will then override the first-in-first-out order and be returned earlier. reserve() only returns one at a time (watch() just says where you will look for the items), but you can keep calling it to get more. In my last project, I collected 100 items from the queue, fetched data about them all at the same time (from the Twitter API), and then went back to delete all 100 (via the job-ID I stored when I had read them originally). I hope this is useful to you. If you have a request for another post, Aaron, then please let me know what you would like to know more on, and I'll see what I can do!\nMichael - Jun 0, 2012\nIf I have multiple workers watching the same tube, can the same job overlap? Such as if one worker grabs a job, the other worker won't grab the same job right, even if the first worker hasn't finished?\nalister - Jun 0, 2012\nThe same job won't be given to two (or more) workers unless the connection to the worker is dropped before it has been deleted, or unless the time allocated (the TTR) to run the particular job has expired. If it's taking a long time, but the worker is handling it, you can also 'TOUCH' the job, to reset the clock. There's also a writeup of this on the FAQ - https://github.com/kr/beanstalkd/wiki/faq - under \u0026quot;How does TTR work?\u0026quot;.\nLornaJane - Mar 2, 2014\nThanks for this, I'm doing some beanstalk stuff now and finding a LOT of your resources around. Thanks for taking the time :)\nalister - Mar 2, 2014\nHappy to help!\n","link":"https://phpscaling.com/post/doing-the-work-elsewhere-adding-a-job-to-the-queue/","section":"post","tags":["beanstalkd","php","queues","scaling","tools","workers"],"title":"Doing the work elsewhere - Adding a job to the queue"},{"body":"","link":"https://phpscaling.com/tags/queues/","section":"tags","tags":null,"title":"queues"},{"body":"","link":"https://phpscaling.com/series/queues/","section":"series","tags":null,"title":"queues"},{"body":"","link":"https://phpscaling.com/tags/scaling/","section":"tags","tags":null,"title":"scaling"},{"body":"","link":"https://phpscaling.com/tags/workers/","section":"tags","tags":null,"title":"workers"},{"body":"I'm taking a slight diversion now, to show you how the main worker processor runs. There are two parts to it - the actual worker, written in PHP, and the script that keeps running it.\nFor testing with return from the worker, we'll just return a random number. In order to avoid returning a normally used exit value, I've picked a few numbers for our controls, up around the 100 range. By default a 'die()' or 'exit' will return a '0', so we can't use that to act on - though we will use it as a fall-back as a generic error. Ideally, we won't get one, instead we want the code in all the workers to just run as planned, and then have the worker execute a planned restart - and we will just immediately restart. We may also choose to have the worker process specifically stop - and so we'll have an exit code for that. If there are any codes we don't understand, we'll slow the system down with a 'sleep()' to avoid running away with the process.\nThe actual script that is run from the command line is a pretty simple BASH script - all it's got to do is to loop, until it gets a particular set of exit values back.\nSo, if it's an exit value we know, we either\npause, then restart immediately restart exit the loop. If it's any other value, we pause, and restart.\nThe bash command exec $0 $@ will re-run the current script ($0) with the original arguments ($@) - but with the 'exec', replaces the current process with a specified command. Normally, when the shell encounters a command, it forks off a child process to actually execute the command. Using the exec builtin, the shell does not fork, and the command exec'ed replaces the shell.\nSave both the PHP and bash script, and then you can start the script with 'sh runBeanstalkd-worker.sh', run it a few times to see a lot of (deliberate) errors that cause the bash script to pause before restart, immediately restart and finally exit.\nWith this bash script in place, we can now run the script as many times as we need - and it will keep running, until we specifically tell it to exit. As usefully, we can exit the php worker, and have it execute a planned restart - which will clear any overheads that the script may have picked up with memory or resource allocation.\nNext time, we'll put some simple tasks into the queue.\nComments: cole - Jul 2, 2012\nfor the shell script i found i need to put an sh in the exec for it to rerun the script: exec sh $0 $@;\nis this something missing in the above code, or a config issue on my end that will break some key functionality here? sorry, bit of a n00b shell scripter\nalister - Jul 2, 2012\nHi! Questions are good, I'm happy to help out.\nPutting a 'sh' in there is equally valid, but I think that's just the way you are calling it that you need it.\nI'm going to guess that the current directory, where the script you are running is placed, is also on the executable path (run set | grep -a ^PATH and look for a lone ., possibly at the start or end of the PATH). That's actually not recommended to do - see the below URLs, and particularly the notes about security.\nWhen I setup to run the workers, I'll either use the 'sh' on the initial call and a full path to the command, or ./command-name. Either way, with a path to the code (and not just a bare script name), it knows how to be able to call itself, without the extra 'sh'. Since it's already running under a shell script, there's no need to add another - and besides, without it, it's a little easier to fin under its own name in the process list :-)\nThe difference is quite easy to see when you run this script:\n1#!/bin/sh 2 3echo \u0026#34;i\u0026#39;m here\u0026#34; 4echo \u0026#34;$0 $@\u0026#34; 5sleep 2 exec $0 $@ First, we'll run it with ./test.sh (or from another directory, with a full path - eg do/stuff/scripts/test.sh):\n1# ./test.sh param1 p2 p3 2i\u0026#39;m here 3./test.sh param1 p2 p3 4i\u0026#39;m here 5./test.sh param1 p2 p3 6i\u0026#39;m here (.... etc) Now, running it with 'sh':\n1% sh test.sh param1 p2 p3 2i\u0026#39;m here 3test.sh param1 p2 p3 4test.sh: 7: exec: test.sh: not found The $0 in the script needs to be able to re-run the same command, but it doesn't get the 'sh' call. It needs to be able to run what it does get.\nGive this a try as well: watch pstree -Apu\nYou can also add a process-ID at the end there (those are the numbers in brackets) to narrow down the part of the tree to look at.\nAlister\nHere's a couple of peoples thoughts as to why it's not a great idea to have the current directory in the PATH.\nhttp://technonstop.com/dot-slash-meaning-linux http://www.linuxforums.org/forum/miscellaneous/27942-linux-doesnt-automatically-add-current-directory-path.html#post141980 Maxime - Jul 5, 2012\nI've follow the instruction and it's working perfectly when I execute the shell script in the console \u0026quot;./worker.sh\u0026quot;. However I want this process to be detached from the console so I've added a \u0026quot;\u0026amp;\u0026quot; at the end of the command \u0026quot;./app/worker.sh \u0026amp;\u0026quot;. But it's now working anymore, the detach process is ending automatically:\n1[user@host]$ ./workers.sh email \u0026amp; 2[2] 6622 3[user@host]$ 4 5[2]+ Stopped ./workers.sh email 6[user@host]$ Any thoughts ?\nalister - Jul 1, 2012\nArranging for the workers to run is one of the harder parts of a major system. I'll write a post sometime with potential options. In the meantime, here's a few ideas:\nIf you will be keeping an eye on it, you could just run it within 'screen' or the more recent tmux). Both allow processes to keep running while you disconnect. I've had instances of 'screen' running for literally months. Upstart, or init.d scripts aren't too hard to write, and considering they are probably already running most of the daemons on your server, they are a great choice that are already available. If you need to run more than one worker on the same machine then you would need multiple copies, so it's not so useful if you are going to run more than a few. If you will be running many copies (and I've done more than 50 identical workers before on the same machine), then supervisord has the max number of workers as a simple number in the configuration, as as many as you need, up to the limit can be started. maxime - Jul 1, 2012\nActually it was because my PHP script was echoing some data and you can't run the shell script without redirecting the output.\n[user@host\\]$ ./workers.sh email \u0026gt; /dev/null 2\u0026gt; /dev/null \u0026amp;\nStanley - Aug 3, 2012\nThanks for the article series. It is very helpful. I have some questions after reading your articles. How should we decide whether to do a planned pause/restart or planned restart?\nAnd what are the criteria used to trigger the planned restart and planned complete exit?\nalister - Aug 4, 2012\nThat depends very much on what sort of things your workers are doing. You might decide that if a worker has run, but there's nothing for them to do, you'll let the shell script pause for a while before it restarts. Equally, you might also elect to just have the beanstalk client block, waiting for the next job to become available - in that case, you might not have need of any planned pauses in the shell script at all.\nThe only difference between a planned and unplanned restart is that an unplanned restart was probably a problem, and so waiting a little while before you try again stops things from spiraling out of control by running too quickly.\nHow feasible is a daemon written in PHP, using ignore_user abort and set_time_limit(0) - PHP Solutions - Developers Q \u0026amp; A - Aug 1, 2013\n[...] Addition: I’ve blogged on a Bash/PHP (or other language) pairing so that you can very easily loop in the PHP script, then exit to restart immediately, or pause for a while – Doing the work elsewhere — Sidebar running the worker. [...]\n","link":"https://phpscaling.com/post/doing-the-work-elsewhere-sidebar-running-the-worker/","section":"post","tags":["beanstalkd","php","queues","scaling","workers"],"title":"Doing the work elsewhere - Sidebar - running the worker"},{"body":"The use of Beanstalkd as a queueing system What is an asynchronous queue The classic wikipedia quote (Message queue)\nIn computer science, message queues and mailboxes are software-engineering components used for interprocess communication, or for inter-thread communication within the same process. They use a queue for messaging - the passing of control or of content. Group communication systems provide similar kinds of functionality.\nSo one part of a system puts a message into a queue for another part to read from, and then act upon. The asynchronous nature means that each side is otherwise independent from the other, and does not wait for a response. That independence is an important part of the nature of the system though - and we'll see later how some of the more advanced functionality for our software of choice here can give some extraordinary flexibility to what can be done.\nWhy use a queuing system? You'd be surprised how few things need to happen right now - you go and buy a fancy coffee, and they write your order down, and put it into the queue for the Barista to make it. That disconnected set of actions works exceeding well for such distributed system (see Starbucks Does Not Use Two-Phase Commit)\nIn much the same way as you not getting your coffee till it's made, what about web-sites that have to fetch (or produce) information. A couple of the simpler examples are when you've uploaded an image onto Flickr.com. That image has to be stored, and then resized into several files. If it's a large image though, it would take some time, and a lot of resources to be able to do that while you waited - time that you're left twiddling your thumbs. Instead, it returns immediately, and tells you that the image is being handled in the background - and in a few seconds, or maybe minutes, it shows up on your page.\nHow about waiting a few seconds for other information? How about, when you login to a social media website, it returns a simple webpage immediately with what it's got to hand, but then in the background, checks how many new messages you have, and displays them either by updating the page (with ajax), or when you view a different page. Is it so vital you find out that you have thirty old messages, and a few new ones - right now? For a web-mail system like Gmail, or Yahoo Mail, that is the point - but what about on another kind of site?\nBeanstalkD Beanstalkd is a big to-do list for your distributed application. If there is a unit of work that you want to defer to later (say, sending an email, pushing some data to a slow external service, pulling data from a slow external service, generating high-quality image thumbnails) you put a description of that work, a \u0026quot;job\u0026quot;, into Beanstalkd. Some processes (such as web request handlers), \u0026quot;producers\u0026quot;, put jobs into the queue. Other processes, \u0026quot;workers\u0026quot;, take jobs out of the queue and run them. From the BeanstalkD FAQ\nWhat can it do? I've already mentioned a few ideas for things to have an asynchronous worker do, via a BeanstalkD queue, but there are a number of ways that it can be run, and a number of very useful facilities that BeanstalkD gives a producer of tasks.\nPriorities Simple enough to describe - given more than one task that could be run at a particular time, run the more important. The most urgent priority is 0; the least urgent priority is 4,294,967,295 (2^32).\nTubes This is, in my mind one of the two secret weapons of Beanstalkd - together with a delayed job. Tubes, or 'named queues' can be created at will, and you can use as many different tubes as you want to put jobs into, but those jobs would only be returned to workers that were watching a given tube. Each worker could be watching many, but a single job can only be in a particular tube.\nIf you don't use a particular tube-name, it goes into 'default', but there's a lot of flexibility in sending particular jobs to specific workers, or groups of workers. For example, you could create a tube called 'sql' watched by workers on a database server, or even further limited by role.\nFile uploads can create special problems, unless you have some significant back-end systems, they will generally be uploaded to a front-end webserver and then have to be processed there, or moved on to somewhere else before they can be processed. This is a common event, so how do you make sure that any request to process an image can only be picked up by a particular machine? Send it to a tube named after the hostname of the server! As long as there is a worker process there, it will be picked up, and run. What it does from there, is up to it - it could resize the image, and save it to a local file system, or arrange for the file to be moved to a central file-storage area, and then fire another message into the queue for further processing there.\nAlthough BeanstalkD doesn't (yet) have persistent queues saved to disk, you could also use a tube as a long-term hold. For example, throw a message into a tube called 'overnight-reports' - but don't have a worker pick it up immediately, instead one is only brought up to run the queue tasks in the quiet overnight hours.\nThe potential flexibility is enormous.\nDelays Another of the secret weapons, or killer features of BeanstalkD, is the ability to hold a message within the queue for a defined period before allowing it to be collected, and acted upon. If you have an action that has to be checked repeatedly, for example, has a particular person come online? then you can fire a number of identical tasks into the queue and allow them to slowly come out as the time passes.\nIt can also be useful to not do everything at once - maybe setting a lower-priority task that would run a few seconds after someone logs in - for example, updating an internal status or record - or checking for lesser-requested information.\nHow to use Although BeanstalkD allows a large amount of information to go into the job-specification (the information that is held in the queue and passed between the producers and workers), I find that a simple string can hold at least a reference to what is required. I take my lead from URLs - and use them to direct the action to be run, and a few parameters as needed. For example - imagine the following strings being sent to a BeanstalkD worker, which it decodes and runs as a task:\n/tasks/image/resize/filename/example.jpg /tasks/image/resize/filename/example.jpg/sizeX/640/sizeY/480 /tasks/image/move/from/web1/to/centralstore/filename/example.jpg /tasks/member/logintasks/id/12345 /tasks/event/add/id/12345/event/27 /tasks/mail/fetchcounts/id/12345 /tasks/mail/check-for-disallowed/id/596583405 Sending simple messages like these would require very little setup from the producer's side, and can be quite easily parsed by any worker process to pass on to a given function. In these examples (some of which I've used myself in live code), the path refers to a Zend Framework layout of module/controller/action \u0026amp; parameters. Rather than sending large amounts of text for the actual contents of a mail message (in the last example path), we simply refer to a record in the database for simplicity. Similarly for an image filename in the first item.\nNext time: Following articles in this series will show code to insert some messages into the queue. From there, I'll show you how to have a worker keep running reliably and pick and run the jobs as required.\n","link":"https://phpscaling.com/post/doing-the-work-elsewhere-asynchronous-message-queues/","section":"post","tags":["advanced","beanstalkd","php","queues","scaling","workers"],"title":"Doing the work elsewhere - Asynchronous Message Queues"},{"body":"Phew. That would have been embarrassing if I'd not passed my ZCE on Thursday afternoon (Jun 4th, 2009).\n","link":"https://phpscaling.com/post/me-instanceof-zce-true/","section":"post","tags":["funemployment","me","Aside","quick","zce","zce"],"title":"($me instanceOf ZCE) === true"},{"body":"","link":"https://phpscaling.com/tags/zce/","section":"tags","tags":null,"title":"zce"},{"body":"","link":"https://phpscaling.com/series/zce/","section":"series","tags":null,"title":"zce"},{"body":"Just a quick note on what is going to be posted in the next few weeks – I've got a few significant pieces in mind for various topics – including:\nDoing the work elsewhere – asynchronous queues This is going to be a series of articles – and to support it, I'm rewriting some code that I had originally wrote for my last job (v2, and so significantly improved over the original). First though, I'll tell you what is planned, and just how asynchronous queues are used and how they can be incredibly useful for scaling up any significant website, and not just in the obvious ways.\nMail queuing, on a vast scale Dave Marshall has just posted an entry on Using message queues to improve user experience where he queues up some emails in order to spool them out over the course of a few minutes. For the last 18 months, I'd been doing something very similar, on a far larger scale, with PEAR's Mail_Queue. I'll show you how I did it and I'll show you how the messages were generated quickly, and how they could be sent out – and more importantly without destroying the system it was running on. As a bonus, I'll show you how if you run some form of internal mail system, you could save gigabytes of database space and give yourself vastly more flexibility.\nSelf stubbing mocks Using the Mocking functionality in PHPunit \u0026amp; Simpletest can be complicated with the various calls that are required. There's just not much documentation around the PHP-world on how to run it. Another method, which can be easier to understand, is self-stubbing – putting your code into the test class. I'll show some examples of how to do that.\nFinally, I'm going to be doing my ZCE exam in the next week or two – quite possibly on Thursday 3rd June (2009). Keep a close eye here for the results, and a follow-up.\n","link":"https://phpscaling.com/post/upcoming-posts-keep-watching/","section":"post","tags":["php","quick"],"title":"Upcoming posts - keep watching"},{"body":"Back at the PHP London Conference at the end of February, iBuildings was offering a little test, with prize for people that could do well answering the sort of questions that are on the ZCE exam. Never one to turn down something useful for free, I took ten minutes to answer the eight questions. A few weeks later, I get an email from them/Zend to say I'd won the chance to take an exam – ZCE, or ZFE (Zend Framework). Although I use ZF, I don't know it well enough to begin to pass any exam, so as I've still not had the chance to take it, I figured, why not take it on their dime?\nAbout 14 months ago, I'd bought 5 tries on the PHPArch-based 'Vulcan' test prep exam. Today, I've come back to it, and gone through it again. Like last time, the test (practice and real) is scheduled to take up to 90 minutes, but I had whipped through them all in 45 minutes, I have finished the 70 questions.\nI'm amused by the fact the only part of this I failed was 'Basic Language'. The first time around it was design patterns. Either way, now I've got some time, I'm going to schedule the test for quite possibly later this week and see about getting the paperwork for it.\nIt's also still 7 'EXCELLENT's, and a fail – just in different places :-)\nCategory Grade XML \u0026amp; Web Services PASS Arrays PASS Web Features EXCELLENT Basic Language FAIL Streams and Network Programming PASS Database Access PASS String Manipulation and Regular Expressions EXCELLENT PHP 4/5 differences EXCELLENT Security EXCELLENT OOP EXCELLENT Functions EXCELLENT Design EXCELLENT Overall ","link":"https://phpscaling.com/post/i-laugh-at-your-zce-exam-prep-tests-2/","section":"post","tags":["me","php","zce"],"title":"I laugh at your ZCE exam prep tests #2"},{"body":"Well go figure. I’ve just won $50 (Canadian, that’s about $3000 USD by now) of books and ‘stuff’ from PHP Arch, care of its publisher, Marco Tabini's, blog.\nHe’d put a little puzzle up last night, some long numbers, and a few short. I recognised them as almost ISBNs – it wasn’t hard to figure them as having dropped a zero from the front, making them “php|architect’s Guide to Programming with Zend Framework” and “php|architect’s Zend PHP 5 Certification Study Guide, 2nd Edition”. From there, guessing the other numbers were page, line and word counts was easy.\nSo, what should I buy? I’ve already got a subscription to the magazine – PDF edition (it’s so much easier to ship bits over the atlantic...).\n","link":"https://phpscaling.com/post/riddled-me-that/","section":"post","tags":["php"],"title":"Riddled me that"},{"body":"see my previous post on the topic, #1.\nMy last post ended up more as a how-to than what-to. This time, I'll say why you should have local copies of the documentation for most of the tools you use. I'll also tell you the sort of things I always have handy as well.\nGetting a local copy of php.net - and getting installed as an apache vhost and updated (probably weekly) is some effort, but well worth it. I've said it before, but PHP.net is the best language reference site that I've seen. It's kept up to date (sometimes ahead of the code releases in fact) and while the notes that are added to it can sometimes confuse, as much as help, when they do help, they will really make the difference.\nI don't tend to buy many PHP books, because what can they do besides re-iterate what is is already there?\nThe most important thing to bear in mind though is not to just have the documentation there to read - you have to know what is available. Projects like PHP, the Zend Framework and PHPUnit have a lot of parts - and knowing that they have things - even if you don't know how they work right now, can save you days or weeks of effort.\nIt's for that reason that you need to at least scan over all the the docs you have - and indeed for all the libraries and tools that you use. Even I don't read everything and expect to remember it all - but I remember enough to recognise that a paticular tool might have something to help - maybe PHP has something to search the values in an array (http://php.net/array-search), or can use Oracle, or Ldap, or Memcached, or that Zend Framework can let you easily loop over maildirs (or an mbox) to get each mail from within it. If you don't read the manual - at least skimming over it, you would never know that functionality exists, and you inevitably end up reimplementing other people's already debugged code. That's a waste of your time.\nSo, take an hour now, and assemble a directory to put these docs into, and read through them - not everything, but at least look at headers of every section, just to get an idea of what is available and maybe go back and read up some more on things that may be useful to you. If something isn't so interesting to you now, do bear in mind, your next project, or job, might change that.\nAbove all, keep learning. Never stop.\nA few of the library docs I have handy - and how PEAR library I generally just download the latest many-HTML-page tar.bz2 file from http://pear.php.net/manual/ and drop it into my docs directory.\nNow obsolete The PEAR library has been completely replaced with Composer packages.\nZend Framework For ZF, there's two sets of documentation - the main docs, and the generated API. Also, since they are released as versions, with the code, rather than continually upgraded. Downloading and unpacking both styles of documentation is easy, and then I'll just add a quick index.html file in the base to quickly bounce into either the manual, or API.\nNow obsolete There are newer versions of the Zend Framework (now called Laminas), but Symfony or Laravel are the usual choices for new development.\nPHPunit or SimpleTest Simpletest is easy, copy the documentation from the source tar-ball. While the team that develops it is getting quickly obsoleted by PHP Unit, they are still ahead on mocking and web-form testing. There are also some ideas somewhat buried in the online docs for upcoming features.\nFor updating the phpunit docs, I've taken to checking out a copy of the docs and running the build script to get the HTML, and then like ZF, having a redirect [in this case, a header('Location: ...')] to bounce the browser into the right place to start reading. An Apache alias or just an easy link from the top level into the generated docs are other trivial ways to do that.\nSubversion - the book http://svnbook.red-bean.com/ and of course, they have the book sources in subversion Probably as easy to just download the multi-file tarball occasionally.\nAlthough TortoiseSvn, and other tools like it make the every day check-ins and updates easy, I know I'll still hit the manual for branching for a while yet.\nNow obsolete Git is almost the default choice for code versioning now.\nYour own code You thought you were going to get away without a reminder to write your own documentation? Wrong!\nPhp Documentor phpXref - A bit of an old-timer, it's actually written in perl, but it will link parts of your code around a large number of HTML pages - useful for code exploration. The Pear PHP_DocBlockGenerator is also useful to generate the place-holders in a newly written file that might be a little light on function docblocks, though it does tend to go a little overboard with variables. If there are docblocks - even empty ones, it won't change them, but it will add function parameter names for you to go back and explain what they are, and why they are there.\nOne little optimisation I've done with a project I'm working on is that since the site is some 45MB - most of that pure PHP (library) code, I don't bother running the document generation unless a PHP file has changed.\nI hope those help, but remember - you have to know where to look for help - so read!\n","link":"https://phpscaling.com/post/always-have-up-to-date-documentation-part-2/","section":"post","tags":["php","tools","zend framework"],"title":"Always have up to date documentation, part #2"},{"body":"","link":"https://phpscaling.com/series/php-documentation/","section":"series","tags":null,"title":"php-documentation"},{"body":"","link":"https://phpscaling.com/tags/zend-framework/","section":"tags","tags":null,"title":"zend framework"},{"body":"When you have a library, like PEAR or Zend Framework – or even just the whole PHP language library – it’s absolutely vital you know what it can do.\nWhat you don’t know can cost you weeks of effort and pain. I found this out (again) today, but it’s not my pain – it’s an employee who was too busy deciding that the Zend Framework wasn’t suitable for a simple cron-script task, he has spent most of the last few weeks duplicating something that is not as good as what I could write – with ZF – in about an hour.\nI'd set him a fairly task - write a PHP script to loop through a maildir, deleting the messages marked (by headers) as spam, and extracting email addresses from a few different kinds of email bounces. This, I gave to a senior ZCE qualified developer, but I had figured that even for a junior developer it would be no more than a couple of days job to help them get into the groove of developing code in a new position.\nIt's been a few weeks since I set him that task, and for the last week - with him working almost full time on it, it's been a couple of hours from completion. He's not impressing me.\nThis afternoon, I got curious, so decided to have a go at it myself. I'd already written the basics of the main loop:\nThat was the crux of the script. There was a little more in the ParseBounces class to look at the headers and body to return the appropriate email addresses, but it's about that simple.\nI didn't know enough about the Zend_Mail_Storage_Maildir class to be able to delete the message - but then, I expected a smart guy to go and fill in the blanks - and here there was just two - delete a message from a Maildir, and update a database record (which is pretty easy).\nWhat I have from him, so far, includes an extention to Zend_Mail_Storage_Maildir to peer inside the protected class and another that decodes all the mails in the Maildir (and currently, there's over 20,000) before passing the emails and filenames into the loop where they are acted upon.\nKnow thy tools first Between 5pm and 5:50pm, I updated that script, shown above, to do what it required, including parsing a couple of the message formats (checking for SpamAssassin headers, a header made by the local MTA -Exim - and parsing a multi-part mime delivery failure message).\nSpot the differences:\nThat didn't actually take me 50 minutes - I spent the first 15 minutes reading the APIs to see what 'Zend_Mail_Storage_Maildir_Writeable' could do, that 'Zend_Mail_Storage_Maildir' could not. It was fairly safe to assume it had to do with change the contents of a maildir. Indeed it can - create new folders, rename them, move messages around, and delete them.\nSo, I've spent less than an hour and changing a few lines to code to do what has taken him weeks to do. While there are some thing I can add - parsing more formats of bounce messages would be high on the list, what is there right now solves probably 80% of the task, and can be added to pretty trivially to do the rest. because it's not parsing the entire directory full of messages first, it's going to take less time, and less memory - and if there's a problem, I can simply kill it part way, and it will have achieved something useful - and then will be able to pick up from where it left off.\nI've not told him yet. But it's amazing what you can pickup from reading the manual, and the code.\n","link":"https://phpscaling.com/post/know-thy-tools-first-of-all/","section":"post","tags":["php","zend framework"],"title":"Know thy tools first of all"},{"body":"As I mentioned in my second post, ZCE prep – and dumb tests - about open book tests (like Brainbench), having a copy of all the relevant documentation can be incredibly useful, if only from a speed issue. Knowing you can just open a new tab and type a few words to get the information on a function, or concept from the manual takes away so many problems.\nI mentioned there that I have a local copy of the main PHP manual – and I wanted to tell you how I keep it, and a couple of other manuals up to date, as well as other documentation.\nMirroring PHP.net It took a little exploring to first find it, but there is a page on how to get make a mirror of the site - http://www.php.net/mirroring.php Although they aren't looking for more mirrors (unless your country doesn't already have a couple around), there's no reason you can't make a private mirror for your office, or just for yourself, as long as you don't update it too often and so cause excess load on the source website. I have a cron-job to sync my own copy once a week on a Sunday afternoon - when I think it was fairly quiet.\nIf you have a linux server (or a Mac OSX would be able to do it as well I expect), and Apache already running, then setting up a virtual host and dragging down a local copy isn't hard. You don't need Mysql running, but if you have SQlite3 installed, you will be also able to use the quick-lookup facility - very handy if you can't remember the order of parameters for strpos - http://php.net/strpos).\nHere's my script I use - taken almost verbatim from the above mirroring.php page:\nIt may look a little more complicated than it it - but in short, it goes to the server at rsync.php.net, and gets the English version of the manual, but not the others, and it also doesn't bother downloading the large source files. In fact, the two largest parts of the site that is mirrored are the manual itself (currently some 44MB) and the manual backend (which includes all the notes, and most usefully, the lookup functions).\nThe first time you download the site, it's going to take a while - it is, currently, 133MB (though the -z in the rsync command line will compress the bytes on the wire, so there is less to download for the same results). However, most of that does not change week to week, and so the rsync protocol comes into it's own by only downloading parts of files that have changed (where they are sufficiently large to make a difference). Here's an example:\nsent 71838 bytes received 9768512 bytes 57045.51 bytes/sec total size is 116855428 speedup is 11.88 Here, the total size it would have downloaded was: 116,855,428 byes, 111MB. However, it only downloaded 9,768,512 (9.3MB) - an over eleven-fold improvement for the same results.\nNext time - What else you should be reading, keeping around for reference, getting updated automatically, and generating from your own codebases!\n","link":"https://phpscaling.com/post/always-have-up-to-date-documentation-part-1/","section":"post","tags":["php","tools"],"title":"Always have up to date documentation, part #1"},{"body":"Well, I've just completed the PHP Arch 'Vulcan' practice test – the first of up to five such practice tests I've purchased. I have quite deliberately not gone through what study materials I have on hand before I took this test (I wanted to get a baseline), but none the less got an 'EXCELLENT' final score, and the same 'Excellent' on seven of the twelve sections the pre-test is broken down into. 'Pass' on four others, and just one 'Fail' on the design (patterns) section. If the real test worked much the same – and with a composite score, rather than having to pass all sections – I doubt I would have a problem to have gotten a passing grade.\nAlthough the test (practice and real) is scheduled to take up to 90 minutes, after 45, I have finished the 70 questions. I'd set about half-dozen to review, but I don't think I changed any of those answers, and so after a little more than 50 minutes – I called for the results.\nIt was much as I expected, with just some occasional requirements to know some parameter orders and specific use (the kinda thing where you test it, and if you didn't get it right first time, I would trivially look it up to check, for functions like sub-string searching). Also several questions on XML handling, which I muddled along with.\nStreams, strings and web-features are something I will have to look at more carefully for next time, but for me the big one is design patterns – it was my only failing section.\nAll in all, I've very happy with this evening's events and the (practice) results.\nCategory Grade XML \u0026amp; Web Services EXCELLENT Arrays EXCELLENT Web Features PASS Basic Language EXCELLENT Streams and Network Programming PASS Database Access EXCELLENT String Manipulation and Regular Expressions PASS PHP 4/5 differences EXCELLENT Security EXCELLENT OOP EXCELLENT Functions PASS Design FAIL ","link":"https://phpscaling.com/post/zce-prep-practice-test-1-2/","section":"post","tags":["me","php","zce"],"title":"ZCE prep - practice test #1"},{"body":" A more recent tool to use I'd suggest using phpcsfixer with one of the default coding standards - PSR-12/PER-Coding-style or @Symfony.\nExtending PHP_CodeSniffer by Raphael Stolt shows how to quite easily add to a tool that will report what parts of your PHP source needs a clean-up, from the built in 'sniffs' for coding standards, and now adding to that for some slightly more opinionated choices on the maximum number of lines per function, or functions per class.\nThis could be a start of a whole collection of additional classes for additional checks.\nThe only challenge I see at the moment (and I don't think it's a big one, though I've not tried it, so it might not even exist, I've just not had the opportunity to look yet) is having to put code into the PEAR directory path, since that is where PHP_CodeSniffer is looking for the base library. I do see that they use long PEAR class-names, so it may just a matter of having the phpcs tool look elsewhere for the base coding style class.\n[edit: 18:06] Aha, some investigation - and copious use of echo's in the phpcs script, and /usr/share/php/PHP/CodeSniffer.php class:\nCreating your own standards, and using them from anywhere on the filesystem - not having to link from inside the PEAR/PHP/CodeSniffer directory:\nThe coding standard class with your extension: The class is called PHP_CodeSniffer_Standards_Example_ExampleCodingStandard - the two 'Example's being the name of the coding standard. The file is called Example/ExampleCodingStandard.php.\nAnd in the subdir: .../Example/Standards/ - Raphael's ToManyMethodsSniff.php and MethodLengthSniff.php files from his post.\nTo call it:\n","link":"https://phpscaling.com/post/a-useful-idea-for-helping-to-enforce-php-code-standards/","section":"post","tags":["php","PHP_CodeSniffer","standards"],"title":"A useful idea for helping to enforce PHP code standards"},{"body":"","link":"https://phpscaling.com/tags/php_codesniffer/","section":"tags","tags":null,"title":"PHP_CodeSniffer"},{"body":"","link":"https://phpscaling.com/tags/standards/","section":"tags","tags":null,"title":"standards"},{"body":"","link":"https://phpscaling.com/tags/phplondon07/","section":"tags","tags":null,"title":"phplondon07"},{"body":"","link":"https://phpscaling.com/tags/phplondon08/","section":"tags","tags":null,"title":"phplondon08"},{"body":"This week I'm going to take the first of my PHPArch.com's ZCE prep test – then I'll read the book and see they they expect me to know.\nGoing for the Zend Certification is something I've been thinking of doing for a couple of years, and especially now that it covers PHP5 – and increasingly good practices and security topics. It's not that I need to get the ZCE, I'd go for it , for the intellectual challenge if nothing else. It's also the closest thing I would have to a professional qualification since I completed a HNC computer studies in 1992 – and that was just 1/day week day release over the course of a couple of years.\nOf course, it's not the first PHP test I've taken – last year, just before the PHP London 2007 conference, Allegis had come along to the PHP groups's early-February meeting, to plug their services (and they got business from it, one guy interviewed the following day, a Friday, and started work on the Monday) – but they offered to have anyone that wanted to do the Brainbench test, paid for, by Allegis.\nAt the time, I had just started a couple of days before at a job near Covent Garden, but then left it after a week for a better gig (where I still am now, some 13 months on) – but I took Allegis up on the offer, and finally got the results at the conference. I never did get a copy of the exact numbers, but I was told the headlines, so these may not be exact, but they are certainly in the ballpark.\nTime: 28 minutes (apparently this is very good) Score 4.73 (out of 5.0) Better than 98% of other test-takers. I'm told that the harder the questions you answer, the harder the next questions get – so getting from 4.0 to 5.0 is a lot harder than getting from 3.0 to 4.0 – if I'm wrong about that, then please let me know.\nThe thing is, the Brainbench tests are open-book – they pretty much have to be, you take them at home, though they are against the clock. I can certainly appreciate the logic of it – after all, which serious developer doesn't have an internet connection and a quick bookmark to php.net (and I've look at the other language sites – php.net is by far the best), or at least a copy of the documentation around (.CHM file or just a bunch of HTML pages – or, like I do, a weekly rsynced copy of the php.net manual!). Just as well I did, the tests I've seen always throw in some pointless questions like how to use LDAP, or how to connect to an Oracle database. I've never used either of them, so I don't bother to learn them – but if I did need them, I'd figure it out in a few minutes reading – or, more likely, I'd have some kind of library, like the Zend Framework which did the hard work for me – plus, I'd only end up writing that kind of code once anyway before I threw it into a function and forgot the minutiae.\nIt saddens me when people are too dumb to do well on such a test though – how hard is it to read the manual, at least well enough to know where to refer to for more advice?\nThe ZCE is a closed book exam – or, as I call it (for all the reasons the brain-bench is open-book) – unrealistic. If I can't recall whether the $haystack or $needle come first in in array or string search – it's but a moment to look it up.\nEven though I've listed my scores above, I don't bother to promote myself with them on my CV – indeed Allegis is the only company (recruiter or not) that know them – they did pay for it after all. Because I can get those kind of scores with what I consider so little effort (about half-an-hour's worth in fact), then either the test is bad, or 98% of the other people that gave taken that test are. Frankly, I've got to think it's mostly the latter.\nKeep reading my posts, and I'll tell you want you need to do to ace your tests – and not look a fool when it comes to developing something I might set you.\n","link":"https://phpscaling.com/post/zce-prep-and-dumb-tests-2/","section":"post","tags":["php","phplondon07","phplondon08","zce"],"title":"ZCE prep - and dumb tests"},{"body":"","link":"https://phpscaling.com/tags/subversion/","section":"tags","tags":null,"title":"subversion"},{"body":"I've read http://www.svn-checkout.co.uk/2008/01/19/how-to-release-new-versions-of-websites/ via http://www.lornajane.net/posts/2008/SVN-Deployment-and-a-New-Site and while I consider revision control an essential tool (a few years ago, my job was the only one in the previous five years where I didn't have to install my own RCS), I somewhat disagree on the idea they suggest.\nThat first link, 'how to release new versions of websites', suggests checking out a version of the site as a working copy, (it's certainly something I've done before now), but then it goes on to use the 'svn switch' capability to move between versions (of course, they are also doing in in TortoiseSvn, and there for the live web-server is likely to be running Windows, no way I'd run a server on Windows - not even for testing). There is however some trouble with revision switching - especially on a busy site that has to keep running even while the new version is being put into place. While SVN does atomic commits - the new code goes into the repository all at once, or not, it's harder to do the all at once part on a non-transactional file-system - such as a webserver. The bigger problem is that rolling back will also take time - and in an emergency, the time taken to do something is crucial.\nHere's what I do - Whenever I want a new version to go live, which might be from every couple of days to as often as a couple of times per day, I'll update run the script below with the specific revision number to export (and a date/time, but that's just for easy reference). When it runs, it also symlinks the given version as 'dev' - which is part of the path to the site 'dev.example.com', wham, instant new version, and I can trivially delete the symlink and on the same command line symlink the older version (with 'rm dev \u0026amp;\u0026amp; ln -s 1234.20080102 dev'). After a quick wander around the newly checked out version, maybe run a set of unit tests, just for security, it's just as easy to put a similar 'live' symlink into place with a new link.\nIf there is ever a problem, rolling back to a previous live version is just as easy. Other configuration changes are made within the code on the apache servername, or the machine's own hostname (more useful for CLI scripts, generally run from cron).\nThere are some downsides - with a complete checkout of a 40-some megabyte website (it's mostly the Zend Framework and other libraries from which I use a number of files, though rarely all), it takes a little while (not too long, it's a gigabit link between the repository and the main webserver) and there are also some potential caching issues (Etags are usually based on the file inodes), but as we plan to move to a multi-machine cluster - and the images aren't being served from Apache, but a dedicated image webserver, that's not a significant issue - and even on Apache we don't have Etags enabled (Yslow from Yahoo also suggests that).\nUpdate: Since I first posted this, I've started using Capistrano. Look out for new posts on how to best use that.\nComments: Deployment with Capistrano – the Gotchas | PHP Scaling - Nov 4, 2011\n[...] and entirely workable system – I described something just like this in a previous post: “SVN checkouts vs exports for live versions”. That was written and used before I was deploying to multiple machines however – and had to [...]\n","link":"https://phpscaling.com/post/svn-checkouts-vs-exports-for-live-versions/","section":"post","tags":["best-practice","linux","subversion","tools"],"title":"svn checkouts vs exports for live versions"},{"body":"","link":"https://phpscaling.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://phpscaling.com/tags/index/","section":"tags","tags":null,"title":"index"}]